

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>image_classifier_3d.models package &mdash; Image Classifier 3D 0.1.2 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="image_classifier_3d.utils package" href="image_classifier_3d.utils.html" />
    <link rel="prev" title="image_classifier_3d.data_loader package" href="image_classifier_3d.data_loader.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> Image Classifier 3D
          

          
          </a>

          
            
            
              <div class="version">
                0.1.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="installation.html#stable-release">Stable release</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#from-sources">From sources</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">Package modules</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="image_classifier_3d.html">image_classifier_3d package</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="image_classifier_3d.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="image_classifier_3d.bin.html">image_classifier_3d.bin package</a></li>
<li class="toctree-l4"><a class="reference internal" href="image_classifier_3d.data_loader.html">image_classifier_3d.data_loader package</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">image_classifier_3d.models package</a></li>
<li class="toctree-l4"><a class="reference internal" href="image_classifier_3d.utils.html">image_classifier_3d.utils package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="image_classifier_3d.html#submodules">Submodules</a></li>
<li class="toctree-l3"><a class="reference internal" href="image_classifier_3d.html#module-image_classifier_3d.proj_tester">image_classifier_3d.proj_tester module</a></li>
<li class="toctree-l3"><a class="reference internal" href="image_classifier_3d.html#module-image_classifier_3d.proj_trainer">image_classifier_3d.proj_trainer module</a></li>
<li class="toctree-l3"><a class="reference internal" href="image_classifier_3d.html#module-image_classifier_3d">Module contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#get-started">Get Started!</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#deploying">Deploying</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="math.html">Math notation example</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Image Classifier 3D</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="modules.html">image_classifier_3d</a> &raquo;</li>
        
          <li><a href="image_classifier_3d.html">image_classifier_3d package</a> &raquo;</li>
        
      <li>image_classifier_3d.models package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/image_classifier_3d.models.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <div class="rst-breadcrumbs-buttons" role="navigation" aria-label="breadcrumb navigation">
      
        <a href="image_classifier_3d.utils.html" class="btn btn-neutral float-right" title="image_classifier_3d.utils package" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="image_classifier_3d.data_loader.html" class="btn btn-neutral float-left" title="image_classifier_3d.data_loader package" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
  </div>
  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="image-classifier-3d-models-package">
<h1>image_classifier_3d.models package<a class="headerlink" href="#image-classifier-3d-models-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-image_classifier_3d.models.build_classifier">
<span id="image-classifier-3d-models-build-classifier-module"></span><h2>image_classifier_3d.models.build_classifier module<a class="headerlink" href="#module-image_classifier_3d.models.build_classifier" title="Permalink to this headline">¶</a></h2>
<p>PyTorch Lightning model class for mitotic classifier</p>
<dl class="class">
<dt id="image_classifier_3d.models.build_classifier.Mitotic_Classifier">
<em class="property">class </em><code class="sig-prename descclassname">image_classifier_3d.models.build_classifier.</code><code class="sig-name descname">Mitotic_Classifier</code><span class="sig-paren">(</span><em class="sig-param">hparams</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/build_classifier.html#Mitotic_Classifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_lightning.core.lightning.LightningModule</span></code></p>
<dl class="method">
<dt id="image_classifier_3d.models.build_classifier.Mitotic_Classifier.configure_optimizers">
<code class="sig-name descname">configure_optimizers</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/build_classifier.html#Mitotic_Classifier.configure_optimizers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.configure_optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>Choose what optimizers and learning-rate schedulers to use in your optimization.
Normally you’d need one. But in the case of GANs or similar you might have multiple.</p>
<dl>
<dt>Return:</dt><dd><p>Any of these 6 options.</p>
<ul class="simple">
<li><p>Single optimizer.</p></li>
<li><p>List or Tuple - List of optimizers.</p></li>
<li><p>Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).</p></li>
<li><p>Dictionary, with an ‘optimizer’ key, and (optionally) a ‘lr_scheduler’
key which value is a single LR scheduler or lr_dict.</p></li>
<li><p>Tuple of dictionaries as described, with an optional ‘frequency’ key.</p></li>
<li><p>None - Fit will run without any optimizer.</p></li>
</ul>
</dd>
<dt>Note:</dt><dd><p>The ‘frequency’ value is an int corresponding to the number of sequential batches
optimized with the specific optimizer. It should be given to none or to all of the optimizers.
There is a difference between passing multiple optimizers in a list,
and passing multiple optimizers in dictionaries with a frequency of 1:
In the former case, all optimizers will operate on the given batch in each optimization step.
In the latter, only one optimizer will operate on the given batch at every step.</p>
<p>The lr_dict is a dictionary which contains scheduler and its associated configuration.
It has five keys. The default configuration is shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scheduler&#39;</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span> <span class="c1"># The LR schduler</span>
    <span class="s1">&#39;interval&#39;</span><span class="p">:</span> <span class="s1">&#39;epoch&#39;</span><span class="p">,</span> <span class="c1"># The unit of the scheduler&#39;s step size</span>
    <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="c1"># The frequency of the scheduler</span>
    <span class="s1">&#39;reduce_on_plateau&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="c1"># For ReduceLROnPlateau scheduler</span>
    <span class="s1">&#39;monitor&#39;</span><span class="p">:</span> <span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="c1"># Metric for ReduceLROnPlateau to monitor</span>
    <span class="s1">&#39;strict&#39;</span><span class="p">:</span> <span class="kc">True</span> <span class="c1"># Whether to crash the training if `monitor` is not found</span>
<span class="p">}</span>
</pre></div>
</div>
<p>If user only provides LR schedulers, then their configuration will set to default as shown above.</p>
</dd>
<dt>Examples:</dt><dd><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># most cases</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">opt</span>

<span class="c1"># multiple optimizer case (e.g.: GAN)</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span>

<span class="c1"># example with learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">discriminator_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">discriminator_sched</span><span class="p">]</span>

<span class="c1"># example with step-based learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">gen_sched</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;scheduler&#39;</span><span class="p">:</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">gen_opt</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span>
                 <span class="s1">&#39;interval&#39;</span><span class="p">:</span> <span class="s1">&#39;step&#39;</span><span class="p">}</span>  <span class="c1"># called after each training step</span>
    <span class="n">dis_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># called every epoch</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">gen_opt</span><span class="p">,</span> <span class="n">dis_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">gen_sched</span><span class="p">,</span> <span class="n">dis_sched</span><span class="p">]</span>

<span class="c1"># example with optimizer frequencies</span>
<span class="c1"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span>
<span class="c1"># https://arxiv.org/abs/1704.00028</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">n_critic</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="p">{</span><span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="n">dis_opt</span><span class="p">,</span> <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="n">n_critic</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="n">gen_opt</span><span class="p">,</span> <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
    <span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<p>Note:</p>
<blockquote>
<div><p>Some things to know:</p>
<ul>
<li><p>Lightning calls <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> and <code class="docutils literal notranslate"><span class="pre">.step()</span></code> on each optimizer
and learning rate scheduler as needed.</p></li>
<li><p>If you use 16-bit precision (<code class="docutils literal notranslate"><span class="pre">precision=16</span></code>), Lightning will automatically
handle the optimizers for you.</p></li>
<li><p>If you use multiple optimizers, <a class="reference internal" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.training_step" title="image_classifier_3d.models.build_classifier.Mitotic_Classifier.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will have an additional
<code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p></li>
<li><p>If you use LBFGS Lightning handles the closure function automatically for you.</p></li>
<li><p>If you use multiple optimizers, gradients will be calculated only
for the parameters of current optimizer at each training step.</p></li>
<li><p>If you need to control how often those optimizers step or override the
default <code class="docutils literal notranslate"><span class="pre">.step()</span></code> schedule, override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code> hook.</p></li>
<li><p>If you only want to call a learning rate scheduler every <code class="docutils literal notranslate"><span class="pre">x</span></code> step or epoch,
or want to monitor a custom metric, you can specify these in a lr_dict:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scheduler&#39;</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span>
    <span class="s1">&#39;interval&#39;</span><span class="p">:</span> <span class="s1">&#39;step&#39;</span><span class="p">,</span>  <span class="c1"># or &#39;epoch&#39;</span>
    <span class="s1">&#39;monitor&#39;</span><span class="p">:</span> <span class="s1">&#39;val_f1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
</div></blockquote>
</dd></dl>

<dl class="method">
<dt id="image_classifier_3d.models.build_classifier.Mitotic_Classifier.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/build_classifier.html#Mitotic_Classifier.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>, however in Lightning you want this to define
the operations you want to use for prediction (i.e.: on a server or as a feature extractor).</p>
<p>Normally you’d call <code class="docutils literal notranslate"><span class="pre">self()</span></code> from your <a class="reference internal" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.training_step" title="image_classifier_3d.models.build_classifier.Mitotic_Classifier.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> method.
This makes it easy to write a complex system for training with the outputs
you’d want in a prediction setting.</p>
<p>You may also find the <code class="xref py py-func docutils literal notranslate"><span class="pre">auto_move_data()</span></code> decorator useful
when using the module outside Lightning in a production setting.</p>
<dl>
<dt>Args:</dt><dd><p><a href="#id1"><span class="problematic" id="id2">*</span></a>args: Whatever you decide to pass into the forward method.
<a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs: Keyword arguments are also possible.</p>
</dd>
<dt>Return:</dt><dd><p>Predicted output</p>
</dd>
<dt>Examples:</dt><dd><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># example if we were using this model as a feature extractor</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">feature_maps</span>

<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>

    <span class="c1"># ...</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># splitting it this way allows model to be used a feature extractor</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModelAbove</span><span class="p">()</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">get_request</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">server</span><span class="o">.</span><span class="n">write_results</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># -------------</span>
<span class="c1"># This is in stark contrast to torch.nn.Module where normally you would have this:</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="image_classifier_3d.models.build_classifier.Mitotic_Classifier.test_dataloader">
<code class="sig-name descname">test_dataloader</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/build_classifier.html#Mitotic_Classifier.test_dataloader"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.test_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for testing.</p>
<p>The dataloader you return will not be called every epoch unless you set
<a href="#id5"><span class="problematic" id="id6">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch`</span></a> to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p>process and split in <code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></p></li>
<li><p><a class="reference internal" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.train_dataloader" title="image_classifier_3d.models.build_classifier.Mitotic_Classifier.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.val_dataloader" title="image_classifier_3d.models.build_classifier.Mitotic_Classifier.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.test_dataloader" title="image_classifier_3d.models.build_classifier.Mitotic_Classifier.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<dl>
<dt>Note:</dt><dd><p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</dd>
<dt>Return:</dt><dd><p>Single or multiple PyTorch DataLoaders.</p>
</dd>
<dt>Example:</dt><dd><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
</dd>
<dt>Note:</dt><dd><p>If you don’t need a test dataset and a <a class="reference internal" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.test_step" title="image_classifier_3d.models.build_classifier.Mitotic_Classifier.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>, you don’t need to implement
this method.</p>
</dd>
<dt>Note:</dt><dd><p>In the case where you return multiple test dataloaders, the <a class="reference internal" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.test_step" title="image_classifier_3d.models.build_classifier.Mitotic_Classifier.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="image_classifier_3d.models.build_classifier.Mitotic_Classifier.test_epoch_end">
<code class="sig-name descname">test_epoch_end</code><span class="sig-paren">(</span><em class="sig-param">outputs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/build_classifier.html#Mitotic_Classifier.test_epoch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.test_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of a test epoch with the output of all test steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>outputs: List of outputs you defined in <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step_end()</span></code>, or if there</dt><dd><p>are multiple dataloaders, a list containing a list of outputs for each dataloader</p>
</dd>
</dl>
</dd>
<dt>Return:</dt><dd><p>None</p>
</dd>
<dt>Note:</dt><dd><p>If you didn’t define a <a class="reference internal" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.test_step" title="image_classifier_3d.models.build_classifier.Mitotic_Classifier.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>, this won’t be called.</p>
</dd>
<dt>Examples:</dt><dd><p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="c1"># do something with the outputs of all test batches</span>
    <span class="n">all_test_preds</span> <span class="o">=</span> <span class="n">test_step_outputs</span><span class="o">.</span><span class="n">predictions</span>

    <span class="n">some_result</span> <span class="o">=</span> <span class="n">calc_all_results</span><span class="p">(</span><span class="n">all_test_preds</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">some_result</span><span class="p">)</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each test step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="n">final_value</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">dataloader_outputs</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">test_step_out</span> <span class="ow">in</span> <span class="n">dataloader_outputs</span><span class="p">:</span>
            <span class="c1"># do something</span>
            <span class="n">final_value</span> <span class="o">+=</span> <span class="n">test_step_out</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;final_metric&#39;</span><span class="p">,</span> <span class="n">final_value</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="image_classifier_3d.models.build_classifier.Mitotic_Classifier.test_step">
<code class="sig-name descname">test_step</code><span class="sig-paren">(</span><em class="sig-param">batch</em>, <em class="sig-param">batch_idx</em>, <em class="sig-param">optimizer_idx=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/build_classifier.html#Mitotic_Classifier.test_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.test_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Operates on a single batch of data from the test set.
In this step you’d normally generate examples or calculate anything of interest
such as accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>batch (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]):</dt><dd><p>The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p>
</dd>
</dl>
<p>batch_idx (int): The index of this batch.
dataloader_idx (int): The index of the dataloader that produced this batch</p>
<blockquote>
<div><p>(only if multiple test datasets used).</p>
</div></blockquote>
</dd>
<dt>Return:</dt><dd><p>Any of.</p>
<blockquote>
<div><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><cite>None</cite> - Testing will skip to the next batch</p></li>
</ul>
</div></blockquote>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one test dataloader:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>

<span class="c1"># if you have multiple test dataloaders:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
</pre></div>
</div>
<dl>
<dt>Examples:</dt><dd><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single test dataset</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;test_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple validation datasets, <a class="reference internal" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.test_step" title="image_classifier_3d.models.build_classifier.Mitotic_Classifier.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will have an additional
argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple test datasets</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
</pre></div>
</div>
</dd>
<dt>Note:</dt><dd><p>If you don’t need to validate you don’t need to implement this method.</p>
</dd>
<dt>Note:</dt><dd><p>When the <a class="reference internal" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.test_step" title="image_classifier_3d.models.build_classifier.Mitotic_Classifier.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> is called, the model has been put in eval mode and
PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
to training mode and gradients are enabled.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="image_classifier_3d.models.build_classifier.Mitotic_Classifier.train_dataloader">
<code class="sig-name descname">train_dataloader</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/build_classifier.html#Mitotic_Classifier.train_dataloader"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.train_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement a PyTorch DataLoader for training.</p>
<dl class="simple">
<dt>Return:</dt><dd><p>Single PyTorch <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>.</p>
</dd>
</dl>
<p>The dataloader you return will not be called every epoch unless you set
<a href="#id7"><span class="problematic" id="id8">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch`</span></a> to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p>process and split in <code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></p></li>
<li><p><a class="reference internal" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.train_dataloader" title="image_classifier_3d.models.build_classifier.Mitotic_Classifier.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
</ul>
<dl>
<dt>Note:</dt><dd><p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</dd>
<dt>Example:</dt><dd><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">loader</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="image_classifier_3d.models.build_classifier.Mitotic_Classifier.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="image_classifier_3d.models.build_classifier.Mitotic_Classifier.training_epoch_end">
<code class="sig-name descname">training_epoch_end</code><span class="sig-paren">(</span><em class="sig-param">outputs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/build_classifier.html#Mitotic_Classifier.training_epoch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.training_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the training epoch with the outputs of all training steps.
Use this in case you need to do something with all the outputs for every training_step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">train_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">train_batch</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="n">train_batch</span><span class="p">)</span>
    <span class="n">train_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">training_epoch_end</span><span class="p">(</span><span class="n">train_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>outputs: List of outputs you defined in <a class="reference internal" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.training_step" title="image_classifier_3d.models.build_classifier.Mitotic_Classifier.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>, or if there are</dt><dd><p>multiple dataloaders, a list containing a list of outputs for each dataloader.</p>
</dd>
</dl>
</dd>
<dt>Return:</dt><dd><p>None</p>
</dd>
<dt>Note:</dt><dd><p>If this method is not overridden, this won’t be called.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_step_outputs</span><span class="p">):</span>
    <span class="c1"># do something with all training_step outputs</span>
    <span class="k">return</span> <span class="n">result</span>
</pre></div>
</div>
<p>With multiple dataloaders, <code class="docutils literal notranslate"><span class="pre">outputs</span></code> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each training step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_step_outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">training_step_outputs</span><span class="p">:</span>
        <span class="c1"># do something here</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="image_classifier_3d.models.build_classifier.Mitotic_Classifier.training_step">
<code class="sig-name descname">training_step</code><span class="sig-paren">(</span><em class="sig-param">batch</em>, <em class="sig-param">batch_idx</em>, <em class="sig-param">optimizer_idx=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/build_classifier.html#Mitotic_Classifier.training_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.training_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Here you compute and return the training loss and some additional metrics for e.g.
the progress bar or logger.</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>batch (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]):</dt><dd><p>The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p>
</dd>
</dl>
<p>batch_idx (int): Integer displaying index of this batch
optimizer_idx (int): When using multiple optimizers, this argument will also be present.
hiddens(<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>): Passed in if</p>
<blockquote>
<div><p><a href="#id9"><span class="problematic" id="id10">:paramref:`~pytorch_lightning.trainer.trainer.Trainer.truncated_bptt_steps`</span></a> &gt; 0.</p>
</div></blockquote>
</dd>
<dt>Return:</dt><dd><p>Any of.</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> - The loss tensor</p></li>
<li><p><cite>dict</cite> - A dictionary. Can include any keys, but must include the key ‘loss’</p></li>
<li><p><cite>None</cite> - Training will skip to the next batch</p></li>
</ul>
</dd>
</dl>
<p>In this step you’d normally do the forward pass and calculate the loss for a batch.
You can also do fancier things like multiple forward passes or something model specific.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>If you define multiple optimizers, this step will be called with an additional
<code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Multiple optimizers (e.g.: GANs)</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># do training_step with encoder</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># do training_step with decoder</span>
</pre></div>
</div>
<p>If you add truncated back propagation through time you will also get an additional
argument with the hidden states of the previous step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Truncated back-propagation through time</span>
<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">):</span>
    <span class="c1"># hiddens are the hidden states from the previous truncated backprop step</span>
    <span class="o">...</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">hiddens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">hiddens</span><span class="p">)</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;hiddens&#39;</span><span class="p">:</span> <span class="n">hiddens</span><span class="p">}</span>
</pre></div>
</div>
<dl class="simple">
<dt>Note:</dt><dd><p>The loss value shown in the progress bar is smoothed (averaged) over the last values,
so it differs from the actual loss returned in train/validation step.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="image_classifier_3d.models.build_classifier.Mitotic_Classifier.val_dataloader">
<code class="sig-name descname">val_dataloader</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/build_classifier.html#Mitotic_Classifier.val_dataloader"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.val_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for validation.</p>
<p>The dataloader you return will not be called every epoch unless you set
<a href="#id11"><span class="problematic" id="id12">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch`</span></a> to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>It’s recommended that all data downloads and preparation happen in <code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></p></li>
<li><p><a class="reference internal" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.train_dataloader" title="image_classifier_3d.models.build_classifier.Mitotic_Classifier.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.val_dataloader" title="image_classifier_3d.models.build_classifier.Mitotic_Classifier.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.test_dataloader" title="image_classifier_3d.models.build_classifier.Mitotic_Classifier.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<dl>
<dt>Note:</dt><dd><p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</dd>
<dt>Return:</dt><dd><p>Single or multiple PyTorch DataLoaders.</p>
</dd>
<dt>Examples:</dt><dd><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
</dd>
<dt>Note:</dt><dd><p>If you don’t need a validation dataset and a <a class="reference internal" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.validation_step" title="image_classifier_3d.models.build_classifier.Mitotic_Classifier.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, you don’t need to
implement this method.</p>
</dd>
<dt>Note:</dt><dd><p>In the case where you return multiple validation dataloaders, the <a class="reference internal" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.validation_step" title="image_classifier_3d.models.build_classifier.Mitotic_Classifier.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="image_classifier_3d.models.build_classifier.Mitotic_Classifier.validation_epoch_end">
<code class="sig-name descname">validation_epoch_end</code><span class="sig-paren">(</span><em class="sig-param">outputs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/build_classifier.html#Mitotic_Classifier.validation_epoch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.validation_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the validation epoch with the outputs of all validation steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>outputs: List of outputs you defined in <a class="reference internal" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.validation_step" title="image_classifier_3d.models.build_classifier.Mitotic_Classifier.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, or if there</dt><dd><p>are multiple dataloaders, a list containing a list of outputs for each dataloader.</p>
</dd>
</dl>
</dd>
<dt>Return:</dt><dd><p>None</p>
</dd>
<dt>Note:</dt><dd><p>If you didn’t define a <a class="reference internal" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.validation_step" title="image_classifier_3d.models.build_classifier.Mitotic_Classifier.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, this won’t be called.</p>
</dd>
<dt>Examples:</dt><dd><p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_step_outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">val_step_outputs</span><span class="p">:</span>
        <span class="c1"># do something</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each validation step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">dataloader_output_result</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="n">dataloader_outs</span> <span class="o">=</span> <span class="n">dataloader_output_result</span><span class="o">.</span><span class="n">dataloader_i_outputs</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;final_metric&#39;</span><span class="p">,</span> <span class="n">final_value</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="image_classifier_3d.models.build_classifier.Mitotic_Classifier.validation_step">
<code class="sig-name descname">validation_step</code><span class="sig-paren">(</span><em class="sig-param">batch</em>, <em class="sig-param">batch_idx</em>, <em class="sig-param">optimizer_idx=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/build_classifier.html#Mitotic_Classifier.validation_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.validation_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Operates on a single batch of data from the validation set.
In this step you’d might generate examples or calculate anything of interest like accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>batch (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]):</dt><dd><p>The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p>
</dd>
</dl>
<p>batch_idx (int): The index of this batch
dataloader_idx (int): The index of the dataloader that produced this batch</p>
<blockquote>
<div><p>(only if multiple val datasets used)</p>
</div></blockquote>
</dd>
<dt>Return:</dt><dd><p>Any of.</p>
<blockquote>
<div><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><cite>None</cite> - Validation will skip to the next batch</p></li>
</ul>
</div></blockquote>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode of order</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">()</span>
<span class="k">if</span> <span class="n">defined</span><span class="p">(</span><span class="s1">&#39;validation_step_end&#39;</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step_end</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one val dataloader:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>

<span class="c1"># if you have multiple val dataloaders:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
</pre></div>
</div>
<dl>
<dt>Examples:</dt><dd><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single validation dataset</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;val_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">val_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple val datasets, validation_step will have an additional argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple validation datasets</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
</pre></div>
</div>
</dd>
<dt>Note:</dt><dd><p>If you don’t need to validate you don’t need to implement this method.</p>
</dd>
<dt>Note:</dt><dd><p>When the <a class="reference internal" href="#image_classifier_3d.models.build_classifier.Mitotic_Classifier.validation_step" title="image_classifier_3d.models.build_classifier.Mitotic_Classifier.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-image_classifier_3d.models.densenet">
<span id="image-classifier-3d-models-densenet-module"></span><h2>image_classifier_3d.models.densenet module<a class="headerlink" href="#module-image_classifier_3d.models.densenet" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="image_classifier_3d.models.densenet.DenseNet">
<em class="property">class </em><code class="sig-prename descclassname">image_classifier_3d.models.densenet.</code><code class="sig-name descname">DenseNet</code><span class="sig-paren">(</span><em class="sig-param">n_input_channels=3</em>, <em class="sig-param">conv1_t_size=7</em>, <em class="sig-param">conv1_t_stride=1</em>, <em class="sig-param">no_max_pool=False</em>, <em class="sig-param">growth_rate=32</em>, <em class="sig-param">block_config=(6</em>, <em class="sig-param">12</em>, <em class="sig-param">24</em>, <em class="sig-param">16)</em>, <em class="sig-param">num_init_features=64</em>, <em class="sig-param">bn_size=4</em>, <em class="sig-param">drop_rate=0</em>, <em class="sig-param">num_classes=1000</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/densenet.html#DenseNet"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.densenet.DenseNet" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Densenet-BC model class
Args:</p>
<blockquote>
<div><p>growth_rate (int) - how many filters to add each layer (k in paper)
block_config (list of 4 ints) - how many layers in each pooling block
num_init_features (int) - the number of filters to learn in the first
convolution layer
bn_size (int) - multiplicative factor for number of bottle neck layers</p>
<blockquote>
<div><p>(i.e. bn_size * k features in the bottleneck layer)</p>
</div></blockquote>
<p>drop_rate (float) - dropout rate after each dense layer
num_classes (int) - number of classification classes</p>
</div></blockquote>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="image_classifier_3d.models.densenet.DenseNet.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/densenet.html#DenseNet.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.densenet.DenseNet.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="image_classifier_3d.models.densenet.DenseNet.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#image_classifier_3d.models.densenet.DenseNet.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="image_classifier_3d.models.densenet.generate_model">
<code class="sig-prename descclassname">image_classifier_3d.models.densenet.</code><code class="sig-name descname">generate_model</code><span class="sig-paren">(</span><em class="sig-param">model_depth</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/densenet.html#generate_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.densenet.generate_model" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-image_classifier_3d.models.resnet">
<span id="image-classifier-3d-models-resnet-module"></span><h2>image_classifier_3d.models.resnet module<a class="headerlink" href="#module-image_classifier_3d.models.resnet" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="image_classifier_3d.models.resnet.BasicBlock">
<em class="property">class </em><code class="sig-prename descclassname">image_classifier_3d.models.resnet.</code><code class="sig-name descname">BasicBlock</code><span class="sig-paren">(</span><em class="sig-param">in_planes</em>, <em class="sig-param">planes</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">downsample=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/resnet.html#BasicBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.resnet.BasicBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="attribute">
<dt id="image_classifier_3d.models.resnet.BasicBlock.expansion">
<code class="sig-name descname">expansion</code><em class="property"> = 1</em><a class="headerlink" href="#image_classifier_3d.models.resnet.BasicBlock.expansion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="image_classifier_3d.models.resnet.BasicBlock.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/resnet.html#BasicBlock.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.resnet.BasicBlock.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="image_classifier_3d.models.resnet.BasicBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#image_classifier_3d.models.resnet.BasicBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="image_classifier_3d.models.resnet.Bottleneck">
<em class="property">class </em><code class="sig-prename descclassname">image_classifier_3d.models.resnet.</code><code class="sig-name descname">Bottleneck</code><span class="sig-paren">(</span><em class="sig-param">in_planes</em>, <em class="sig-param">planes</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">downsample=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/resnet.html#Bottleneck"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.resnet.Bottleneck" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="attribute">
<dt id="image_classifier_3d.models.resnet.Bottleneck.expansion">
<code class="sig-name descname">expansion</code><em class="property"> = 4</em><a class="headerlink" href="#image_classifier_3d.models.resnet.Bottleneck.expansion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="image_classifier_3d.models.resnet.Bottleneck.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/resnet.html#Bottleneck.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.resnet.Bottleneck.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="image_classifier_3d.models.resnet.Bottleneck.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#image_classifier_3d.models.resnet.Bottleneck.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="image_classifier_3d.models.resnet.ResNet">
<em class="property">class </em><code class="sig-prename descclassname">image_classifier_3d.models.resnet.</code><code class="sig-name descname">ResNet</code><span class="sig-paren">(</span><em class="sig-param">block</em>, <em class="sig-param">layers</em>, <em class="sig-param">block_inplanes</em>, <em class="sig-param">n_input_channels=1</em>, <em class="sig-param">conv1_t_size=7</em>, <em class="sig-param">conv1_t_stride=1</em>, <em class="sig-param">no_max_pool=True</em>, <em class="sig-param">shortcut_type='B'</em>, <em class="sig-param">widen_factor=1.0</em>, <em class="sig-param">n_classes=400</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/resnet.html#ResNet"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.resnet.ResNet" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="image_classifier_3d.models.resnet.ResNet.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/resnet.html#ResNet.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.resnet.ResNet.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="image_classifier_3d.models.resnet.ResNet.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#image_classifier_3d.models.resnet.ResNet.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="image_classifier_3d.models.resnet.conv1x1x1">
<code class="sig-prename descclassname">image_classifier_3d.models.resnet.</code><code class="sig-name descname">conv1x1x1</code><span class="sig-paren">(</span><em class="sig-param">in_planes</em>, <em class="sig-param">out_planes</em>, <em class="sig-param">stride=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/resnet.html#conv1x1x1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.resnet.conv1x1x1" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="image_classifier_3d.models.resnet.conv3x3x3">
<code class="sig-prename descclassname">image_classifier_3d.models.resnet.</code><code class="sig-name descname">conv3x3x3</code><span class="sig-paren">(</span><em class="sig-param">in_planes</em>, <em class="sig-param">out_planes</em>, <em class="sig-param">stride=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/resnet.html#conv3x3x3"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.resnet.conv3x3x3" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="image_classifier_3d.models.resnet.generate_model">
<code class="sig-prename descclassname">image_classifier_3d.models.resnet.</code><code class="sig-name descname">generate_model</code><span class="sig-paren">(</span><em class="sig-param">model_depth</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/resnet.html#generate_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.resnet.generate_model" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="image_classifier_3d.models.resnet.get_inplanes">
<code class="sig-prename descclassname">image_classifier_3d.models.resnet.</code><code class="sig-name descname">get_inplanes</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/resnet.html#get_inplanes"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.resnet.get_inplanes" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-image_classifier_3d.models.resnet_GN">
<span id="image-classifier-3d-models-resnet-gn-module"></span><h2>image_classifier_3d.models.resnet_GN module<a class="headerlink" href="#module-image_classifier_3d.models.resnet_GN" title="Permalink to this headline">¶</a></h2>
<p>3D ResNet replacing batch normalization with group normalization to train
on batches of images without padding (thus different sizes)</p>
<dl class="class">
<dt id="image_classifier_3d.models.resnet_GN.BasicBlock">
<em class="property">class </em><code class="sig-prename descclassname">image_classifier_3d.models.resnet_GN.</code><code class="sig-name descname">BasicBlock</code><span class="sig-paren">(</span><em class="sig-param">in_planes</em>, <em class="sig-param">planes</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">downsample=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/resnet_GN.html#BasicBlock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.resnet_GN.BasicBlock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="attribute">
<dt id="image_classifier_3d.models.resnet_GN.BasicBlock.expansion">
<code class="sig-name descname">expansion</code><em class="property"> = 1</em><a class="headerlink" href="#image_classifier_3d.models.resnet_GN.BasicBlock.expansion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="image_classifier_3d.models.resnet_GN.BasicBlock.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/resnet_GN.html#BasicBlock.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.resnet_GN.BasicBlock.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="image_classifier_3d.models.resnet_GN.BasicBlock.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#image_classifier_3d.models.resnet_GN.BasicBlock.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="image_classifier_3d.models.resnet_GN.Bottleneck">
<em class="property">class </em><code class="sig-prename descclassname">image_classifier_3d.models.resnet_GN.</code><code class="sig-name descname">Bottleneck</code><span class="sig-paren">(</span><em class="sig-param">in_planes</em>, <em class="sig-param">planes</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">downsample=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/resnet_GN.html#Bottleneck"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.resnet_GN.Bottleneck" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="attribute">
<dt id="image_classifier_3d.models.resnet_GN.Bottleneck.expansion">
<code class="sig-name descname">expansion</code><em class="property"> = 4</em><a class="headerlink" href="#image_classifier_3d.models.resnet_GN.Bottleneck.expansion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="image_classifier_3d.models.resnet_GN.Bottleneck.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/resnet_GN.html#Bottleneck.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.resnet_GN.Bottleneck.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="image_classifier_3d.models.resnet_GN.Bottleneck.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#image_classifier_3d.models.resnet_GN.Bottleneck.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="image_classifier_3d.models.resnet_GN.ResNetGN">
<em class="property">class </em><code class="sig-prename descclassname">image_classifier_3d.models.resnet_GN.</code><code class="sig-name descname">ResNetGN</code><span class="sig-paren">(</span><em class="sig-param">block</em>, <em class="sig-param">layers</em>, <em class="sig-param">block_inplanes</em>, <em class="sig-param">n_input_channels=1</em>, <em class="sig-param">conv1_t_size=7</em>, <em class="sig-param">conv1_t_stride=1</em>, <em class="sig-param">no_max_pool=True</em>, <em class="sig-param">shortcut_type='B'</em>, <em class="sig-param">widen_factor=1.0</em>, <em class="sig-param">n_classes=400</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/resnet_GN.html#ResNetGN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.resnet_GN.ResNetGN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="image_classifier_3d.models.resnet_GN.ResNetGN.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/resnet_GN.html#ResNetGN.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.resnet_GN.ResNetGN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="image_classifier_3d.models.resnet_GN.ResNetGN.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#image_classifier_3d.models.resnet_GN.ResNetGN.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="image_classifier_3d.models.resnet_GN.conv1x1x1">
<code class="sig-prename descclassname">image_classifier_3d.models.resnet_GN.</code><code class="sig-name descname">conv1x1x1</code><span class="sig-paren">(</span><em class="sig-param">in_planes</em>, <em class="sig-param">out_planes</em>, <em class="sig-param">stride=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/resnet_GN.html#conv1x1x1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.resnet_GN.conv1x1x1" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="image_classifier_3d.models.resnet_GN.conv3x3x3">
<code class="sig-prename descclassname">image_classifier_3d.models.resnet_GN.</code><code class="sig-name descname">conv3x3x3</code><span class="sig-paren">(</span><em class="sig-param">in_planes</em>, <em class="sig-param">out_planes</em>, <em class="sig-param">stride=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/resnet_GN.html#conv3x3x3"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.resnet_GN.conv3x3x3" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="image_classifier_3d.models.resnet_GN.generate_model">
<code class="sig-prename descclassname">image_classifier_3d.models.resnet_GN.</code><code class="sig-name descname">generate_model</code><span class="sig-paren">(</span><em class="sig-param">model_depth</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/resnet_GN.html#generate_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.resnet_GN.generate_model" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="image_classifier_3d.models.resnet_GN.get_inplanes">
<code class="sig-prename descclassname">image_classifier_3d.models.resnet_GN.</code><code class="sig-name descname">get_inplanes</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/image_classifier_3d/models/resnet_GN.html#get_inplanes"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#image_classifier_3d.models.resnet_GN.get_inplanes" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-image_classifier_3d.models">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-image_classifier_3d.models" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Jianxu Chen

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>